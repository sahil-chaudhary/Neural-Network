{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dependencies\n",
    "\n",
    "We are going to use only numpy for the execution of Neural Network and those are going to fully connected network.\n",
    "Torch and matplot are going to be used for importing dataset and visualising the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intialization of Layers and Nodes\n",
    "We need a function that takes the architecture of the network(i.e., the number of layers and nodes in each layer) and initializes the weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 shape: (5, 3)\n",
      "b1 shape: (5, 1)\n",
      "W2 shape: (2, 5)\n",
      "b2 shape: (2, 1)\n",
      "W3 shape: (1, 2)\n",
      "b3 shape: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the parameters for the neural network\n",
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(1)  # for reproducibility\n",
    "    parameters = {}\n",
    "    \n",
    "    for l in range(1, len(layer_dims)):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "# Example usage\n",
    "layer_dims = [3, 5, 2, 1]  # 3 input nodes, 5 nodes in the first hidden layer, 2 in the second, 1 output node\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "\n",
    "for key in parameters:\n",
    "    print(f\"{key} shape: {parameters[key].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define activation function such as ReLu, sigmoid, tanh and softmax and their derivatives\n",
    "1. Sigmoid Function: $\\sigma(Z)=\\frac{1}{1+e^{-Z}}$\n",
    "2. ReLU function: $ReLU(z)=\\max(0,z)$\n",
    "3. Tanh Function: $tanh(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}$\n",
    "4. Softmax Function: $sofmax(z_i)=\\frac{e^{z_i}}{\\sum_j e_{z_j}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z))\n",
    "    return expZ / expZ.sum(axis=0, keepdims=True)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def tanh_backward(dA, Z):\n",
    "    return dA * (1 - np.tanh(Z)**2)\n",
    "\n",
    "def softmax_backward(dA, Z):\n",
    "    return dA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"relu\": relu,\n",
    "    \"tanh\": tanh,\n",
    "    \"softmax\": softmax\n",
    "    }\n",
    "\n",
    "activation_functions_backward = {\n",
    "    \"sigmoid\": sigmoid_backward,\n",
    "    \"relu\": relu_backward,\n",
    "    \"tanh\": tanh_backward,\n",
    "    \"softmax\": softmax_backward\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass\n",
    "Use the chosen activation function to pass the information about your datas.\n",
    "1. $a^{(1)}={w^{(1)}}^Tx+b^{(1)}$\n",
    "2. $a^{(i)}={w^{(i)}}^T.\\sigma(a^{(i-1)})+b^{(i)}$ for $i>1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    cache = (A_prev, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "def activate_forward(A_prev, W, b, activation):\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    elif activation == \"tanh\":\n",
    "        A = tanh(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        A = softmax(Z)\n",
    "    \n",
    "    activation_cache = Z\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "Create a set of Loss function which you would wish to minimize for your job\n",
    "1. Mean Squared Error(MSE)\n",
    "Mean Square error is used for regression problems and is given by $MSE=\\frac{1}{m}\\sum_{i=1}^{m}(y_i-\\hat{y_i})^2$.\n",
    "\n",
    "For this exercise, we are only going to use mean squared error and try to minimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(Y_hat, Y):\n",
    "    m = Y.shape[1]\n",
    "    return np.sum((Y_hat - Y)**2) / (2 * m)\n",
    "\n",
    "def mean_squared_error_backward(Y_hat, Y):\n",
    "    m = Y.shape[1]\n",
    "    return (Y_hat - Y) / m\n",
    "\n",
    "# Cross-entropy loss and its derivative\n",
    "def cross_entropy_loss(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = -np.sum(Y * np.log(AL + 1e-8)) / m  # Adding epsilon for numerical stability\n",
    "    return cost\n",
    "\n",
    "def cross_entropy_loss_backward(AL, Y):\n",
    "    return AL - Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation\n",
    "\n",
    "Implement the backward pass using the derivatives of the activation functions and the MSE Loss!\n",
    "Backward Propagation involves computing the gradients of the loss function with respect to the parameters of the neural network. These gradients are then used to update the parameters. We will use the derivative of the activation functions and the MSE loss we 've implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear backward step\n",
    "def linear_backward(dz, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dz, A_prev.T) / m\n",
    "    db = np.sum(dz, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dz)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "#activation_backward\n",
    "def activate_backward(dz, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        dA = sigmoid_backward(dz, activation_cache)\n",
    "    elif activation == \"relu\":\n",
    "        dA = relu_backward(dz, activation_cache)\n",
    "    elif activation == \"tanh\":\n",
    "        dA = tanh_backward(dz, activation_cache)\n",
    "    elif activation == \"softmax\":\n",
    "        dA = softmax_backward(dz, activation_cache)\n",
    "    \n",
    "    return linear_backward(dA, linear_cache)\n",
    "\n",
    "# Full backward Propagation\n",
    "def backward_propagation(Y_hat, Y, caches, activations):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = Y.shape[1] # number of samples\n",
    "    Y = Y.reshape(Y_hat.shape) # after this line, Y is the same shape as Y_hat\n",
    "\n",
    "    # Initializing the backpropagation\n",
    "    dA = mean_squared_error_backward(Y_hat, Y)\n",
    "\n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L - 1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = activate_backward(dA, current_cache, activations[L - 1])\n",
    "\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L - 1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = activate_backward(grads[\"dA\" + str(l + 2)], current_cache, activations[l])\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "This might be the most important step of this whole setup since everything depends on this small step.\n",
    "\n",
    "1. Stochastic Gradient Descent(SGD)\n",
    "2. Momentum\n",
    "3. RMSprop\n",
    "4. Adam\n",
    "\n",
    "Put the formulas for all these optimizers later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the parameters using gradient descent\n",
    "def update_parameters_sgd(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] -= learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] -= learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    return parameters\n",
    "\n",
    "# Update the parameters using Momentum Optimizer\n",
    "def initialize_velocity(parameters):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "    return v\n",
    "\n",
    "def update_parameters_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + (1 - beta) * grads[\"dW\" + str(l + 1)]\n",
    "        v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + (1 - beta) * grads[\"db\" + str(l + 1)]\n",
    "        parameters[\"W\" + str(l + 1)] -= learning_rate * v[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] -= learning_rate * v[\"db\" + str(l + 1)]\n",
    "    return parameters, v\n",
    "\n",
    "# Update the parameters using RMSprop Optimizer\n",
    "def initialize_rmsprop(parameters):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    s = {}\n",
    "    for l in range(L):\n",
    "        s[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        s[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "    return s\n",
    "\n",
    "def update_parameters_rmsprop(parameters, grads, s, beta, learning_rate, epsilon=1e-8):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    for l in range(L):\n",
    "        s[\"dW\" + str(l + 1)] = beta * s[\"dW\" + str(l + 1)] + (1 - beta) * grads[\"dW\" + str(l + 1)]**2\n",
    "        s[\"db\" + str(l + 1)] = beta * s[\"db\" + str(l + 1)] + (1 - beta) * grads[\"db\" + str(l + 1)]**2\n",
    "        parameters[\"W\" + str(l + 1)] -= learning_rate * grads[\"dW\" + str(l + 1)] / np.sqrt(s[\"dW\" + str(l + 1)] + epsilon)\n",
    "        parameters[\"b\" + str(l + 1)] -= learning_rate * grads[\"db\" + str(l + 1)] / np.sqrt(s[\"db\" + str(l + 1)] + epsilon)\n",
    "    return parameters, s\n",
    "\n",
    "# Update the parameters using Adam Optimizer\n",
    "def initialize_adam(parameters) :\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "        s[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        s[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "    \n",
    "    return v, s\n",
    "\n",
    "def update_parameters_adam(parameters, grads, v, s, t, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads[\"dW\" + str(l + 1)]\n",
    "        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - beta1**t)\n",
    "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - beta1**t)\n",
    "        \n",
    "        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * grads[\"dW\" + str(l + 1)]**2\n",
    "        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * grads[\"db\" + str(l + 1)]**2\n",
    "        \n",
    "        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - beta2**t)\n",
    "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - beta2**t)\n",
    "        \n",
    "        parameters[\"W\" + str(l + 1)] -= learning_rate * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)] + epsilon)\n",
    "        parameters[\"b\" + str(l + 1)] -= learning_rate * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)\n",
    "    \n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "Training loop will combine all the components which has been implemented till now to train the neural network on a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnlayer(X,Y, layer_dims, activations, optimizer, learning_rate, beta=0.9, beta1=0.9, beta2=0.999, epsilon=1e-8, num_epochs=10000, print_cost=True):\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    if optimizer == \"sgd\":\n",
    "        pass\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"rmsprop\":\n",
    "        s = initialize_rmsprop(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        # Forward propagation\n",
    "        A = X\n",
    "        caches = []\n",
    "        for l in range(1, len(layer_dims) - 1):\n",
    "            A_prev = A\n",
    "            A, cache = activate_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activations[l - 1])\n",
    "            caches.append(cache)\n",
    "        \n",
    "        # Output layer\n",
    "        Y_hat, cache = activate_forward(A, parameters[\"W\" + str(len(layer_dims) - 1)], parameters[\"b\" + str(len(layer_dims) - 1)], activations[-1])\n",
    "        caches.append(cache)\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = mean_squared_error(Y_hat, Y)\n",
    "        entropy_cost = cross_entropy_loss(Y_hat, Y)\n",
    "        \n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(Y_hat, Y, caches, activations)\n",
    "        \n",
    "        # Update parameters\n",
    "        if optimizer == \"sgd\":\n",
    "            parameters = update_parameters_sgd(parameters, grads, learning_rate)\n",
    "        elif optimizer == \"momentum\":\n",
    "            parameters, v = update_parameters_momentum(parameters, grads, v, beta, learning_rate)\n",
    "        elif optimizer == \"rmsprop\":\n",
    "            parameters, s = update_parameters_rmsprop(parameters, grads, s, beta, learning_rate, epsilon)\n",
    "        elif optimizer == \"adam\":\n",
    "            t = i + 1\n",
    "            parameters, v, s = update_parameters_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon)\n",
    "        \n",
    "        # Print the cost every 100 epochs\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(f\"Cost after epoch {i}: {cost}\")\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-20 11:50:21.878718: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-20 11:50:21.951044: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-20 11:50:22.030156: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-20 11:50:22.101794: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-20 11:50:22.122047: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-20 11:50:22.244295: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-20 11:50:23.706297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (784, 60000)\n",
      "Y_train shape: (10, 60000)\n",
      "X_test shape: (784, 10000)\n",
      "Y_test shape: (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "\n",
    "# Normalize the data\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).T / 255.0\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).T / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "Y_train = to_categorical(Y_train).T\n",
    "Y_test = to_categorical(Y_test).T\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.45000266067040023\n",
      "Cost after epoch 100: 0.4500026551658971\n",
      "Cost after epoch 200: 0.4500026496615385\n",
      "Cost after epoch 300: 0.4500026441573397\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [784, 128, 64, 10]  # 784 input nodes, 2 hidden layers with 128 and 64 nodes, 10 output nodes\n",
    "activations = [\"relu\", \"relu\", \"softmax\"]\n",
    "\n",
    "# Train the model\n",
    "parameters, costs = nnlayer(X_train,Y_train, layer_dims, activations, optimizer=\"sgd\", learning_rate=0.1, beta=0.9, beta1=0.9, beta2=0.999, epsilon=1e-8, num_epochs=2000, print_cost=True)\n",
    "\n",
    "# Plotting the cost function\n",
    "plt.plot(costs)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Iterations (per hundreds)')\n",
    "plt.title('Cost reduction over time')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 11.35%\n"
     ]
    }
   ],
   "source": [
    "def predict(X, parameters, activations):\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "    caches = []\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        A_prev = A\n",
    "        W = parameters['W' + str(l)]\n",
    "        b = parameters['b' + str(l)]\n",
    "        A, cache = activate_forward(A_prev, W, b, activations[l-1])\n",
    "        caches.append(cache)\n",
    "    \n",
    "    return A\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred = predict(X_test, parameters, activations)\n",
    "\n",
    "# Compute accuracy\n",
    "def compute_accuracy(Y_pred, Y_true):\n",
    "    predictions = np.argmax(Y_pred, axis=0)\n",
    "    labels = np.argmax(Y_true, axis=0)\n",
    "    accuracy = np.mean(predictions == labels)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = compute_accuracy(Y_pred, Y_test)\n",
    "print(f\"Test set accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
