{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, w, b):\n",
    "        return np.dot(x, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dz, x, w, b):\n",
    "    #dz is the gradient of the loss function with respect to the output of the linear layer which happens to be same as the input of the activation function\n",
    "    dw = np.dot(x.T, dz)\n",
    "    db = np.sum(dz, axis=0)\n",
    "    dx = np.dot(dz, w.T)\n",
    "    return dx, dw, db\n",
    "\n",
    "def activation_backward(dz, x, activation):\n",
    "    if activation == 'sigmoid':\n",
    "        print(dz.shape, sigmoid(x).shape)\n",
    "        return dz*sigmoid(x)*(1-sigmoid(x))\n",
    "    elif activation == 'relu':\n",
    "        print(dz.shape, relu(x).shape)\n",
    "        return np.where(x>0, dz, 0)\n",
    "    elif activation == 'tanh':\n",
    "        print(dz.shape, tanh(x).shape)\n",
    "        return 1 - np.tanh(x)**2\n",
    "    elif activation == 'softmax':\n",
    "        print(dz.shape, softmax(x).shape)\n",
    "        return dz*softmax(x)*(1-softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class optimiser:\n",
    "    #This will take care of the collection of optimisation algorithms that can be used to update the weights and biases of the neural network. Later, we will compare the performance of different optimisation algorithms on the same neural network and make some conclusions.\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def gradient_descent(self, weights, biases, dw, db):\n",
    "        weights -= self.learning_rate*dw\n",
    "        biases -= self.learning_rate*db\n",
    "        return weights, biases\n",
    "    \n",
    "    def momentum(self, weights, biases, dw, db, beta):\n",
    "        v_w = beta*v_w + (1-beta)*dw\n",
    "        v_b = beta*v_b + (1-beta)*db\n",
    "        weights -= self.learning_rate*v_w\n",
    "        biases -= self.learning_rate*v_b\n",
    "        return weights, biases\n",
    "    \n",
    "    def rmsprop(self, weights, biases, dw, db, beta):\n",
    "        s_w = beta*s_w + (1-beta)*dw**2\n",
    "        s_b = beta*s_b + (1-beta)*db**2\n",
    "        weights -= self.learning_rate*dw/np.sqrt(s_w + 1e-8)\n",
    "        biases -= self.learning_rate*db/np.sqrt(s_b + 1e-8)\n",
    "        return weights, biases\n",
    "    \n",
    "    def adam(self, weights, biases, dw, db, beta1, beta2):\n",
    "        v_w = beta1*v_w + (1-beta1)*dw\n",
    "        v_b = beta1*v_b + (1-beta1)*db\n",
    "        s_w = beta2*s_w + (1-beta2)*dw**2\n",
    "        s_b = beta2*s_b + (1-beta2)*db**2\n",
    "        v_w_corrected = v_w/(1-beta1)\n",
    "        v_b_corrected = v_b/(1-beta1)\n",
    "        s_w_corrected = s_w/(1-beta2)\n",
    "        s_b_corrected = s_b/(1-beta2)\n",
    "        weights -= self.learning_rate*v_w_corrected/np.sqrt(s_w_corrected + 1e-8)\n",
    "        biases -= self.learning_rate*v_b_corrected/np.sqrt(s_b_corrected + 1e-8)\n",
    "        return weights, biases\n",
    "    \n",
    "\n",
    "class plot:\n",
    "    #This class will take care of the plotting of the neural network. This class will have the following methods:\n",
    "    #1. plot_loss : This will plot the loss function of the neural network\n",
    "    #2. plot_accuracy : This will plot the accuracy of the neural network\n",
    "    #3. plot : This will plot both the loss function and the accuracy of the neural network\n",
    "    #4. plot_decision_boundary : This will plot the decision boundary of the neural network. This will be useful for the classification problems. This will help us to visualise how the neural network is making the decision on the basis of the input features.\n",
    "    #5. plot_confusion_matrix : This will plot the confusion matrix of the neural network. This will help us to visualise how the neural network is performing on the basis of the true labels and the predicted labels.\n",
    "    def __init__(self, x, y, parameters):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.parameters = parameters\n",
    "\n",
    "    def plot_loss(self, loss):\n",
    "        plt.plot(loss)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss Function')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_accuracy(self, accuracy):\n",
    "        plt.plot(accuracy)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy')\n",
    "        plt.show()\n",
    "\n",
    "    def plot(self, loss, accuracy):\n",
    "        plt.plot(loss, label='Loss')\n",
    "        plt.plot(accuracy, label='Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss/Accuracy')\n",
    "        plt.title('Loss/Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_decision_boundary(self, x, y, parameters):\n",
    "        pass\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def mean_squared_error(y_pred, y_true):\n",
    "    return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "def cross_entropy_error(y_pred, y_true):\n",
    "    return -np.sum(y_true*np.log(y_pred))\n",
    "\n",
    "\n",
    "def predict(x):\n",
    "    return np.argmax(x, axis=1)\n",
    "    \n",
    "def accuracy(y_pred, y_true):\n",
    "    return np.mean(y_pred == y_true)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_parameters(x, y, weights, biases, a, z, nodes_num, layers_num, activations):\n",
    "    for i in range(layers_num+1):\n",
    "        a['a_'+str(i)] = linear(z['z_'+str(i)], weights['w_'+str(i)], biases['b_'+str(i)])\n",
    "        if activations[i] == 'sigmoid':\n",
    "            z['z_'+str(i+1)] = sigmoid(a['a_'+str(i)])\n",
    "        elif activations[i] == 'relu':\n",
    "            z['z_'+str(i+1)] = relu(a['a_'+str(i)])\n",
    "        elif activations[i] == 'tanh':\n",
    "            z['z_'+str(i+1)] = tanh(a['a_'+str(i)])\n",
    "        elif activations[i] == 'softmax':\n",
    "            z['z_'+str(i+1)] = softmax(a['a_'+str(i)])\n",
    "    print('done with update parameters')\n",
    "    return a, z\n",
    "\n",
    "def train(x, y, weights, biases, a, z, nodes_num, layers_num, activations, learning_rate, epochs):\n",
    "    loss = []\n",
    "    accuracy1 = []\n",
    "    for i in range(epochs):\n",
    "        a, z = update_parameters(x, y, weights, biases, a, z, nodes_num, layers_num, activations)\n",
    "        loss.append(cross_entropy_error(z['z_'+str(layers_num+1)], y))\n",
    "        print('Epoch:', i, 'Loss:', loss[-1])\n",
    "        accuracy1.append(accuracy(predict(z['z_'+str(layers_num+1)]), y))\n",
    "        print('Accuracy:', accuracy1[-1])\n",
    "        dz = activation_backward(z['z_'+str(layers_num+1)] - y, a['a_'+str(layers_num)], activations[layers_num])\n",
    "        for i in range(layers_num, -1, -1):\n",
    "            print('Layer:', i)\n",
    "            dx, dw, db = linear_backward(dz, z['z_'+str(i)], weights['w_'+str(i)], biases['b_'+str(i)])\n",
    "            weights['w_'+str(i)] -= learning_rate*dw\n",
    "            biases['b_'+str(i)] -= learning_rate*db\n",
    "            dz = activation_backward(dx, a['a_'+str(i)], activations[i])\n",
    "    return weights, biases, loss, accuracy1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weights_and_biases_initialisation(x, y, nodes_num, layers_num):\n",
    "    weights = {}\n",
    "    biases = {}\n",
    "    for i in range(layers_num+1):\n",
    "        if i == 0:\n",
    "            weights['w_0'] = np.random.randn(x.shape[1], nodes_num)\n",
    "            biases['b_0'] = np.random.randn(1, nodes_num)\n",
    "        elif i == layers_num:\n",
    "            weights['w_'+str(i)] = np.random.randn(nodes_num, y.shape[1])\n",
    "            biases['b_'+str(i)] = np.random.randn(1, y.shape[1])\n",
    "        else:\n",
    "            weights['w_'+str(i)] = np.random.randn(nodes_num, nodes_num)\n",
    "            biases['b_'+str(i)] = np.random.randn(1, nodes_num)\n",
    "    print('done with weights and biases initialisation')\n",
    "    return weights, biases\n",
    "\n",
    "def z_initialisation(x, y, nodes_num, layers_num):\n",
    "    z={}\n",
    "    for i in range(layers_num+2):\n",
    "        if i == 0:\n",
    "            z['z_0'] = x\n",
    "        else:\n",
    "            z['z_'+str(i)] = np.zeros((x.shape[0], nodes_num))\n",
    "    print('done with z initialisation')\n",
    "    return z\n",
    "\n",
    "\n",
    "def layer_forward_linear_initialisation(x, y, nodes_num, layers_num):\n",
    "    a={}\n",
    "    for i in range(layers_num+1):\n",
    "            a['a_'+str(i)] = np.zeros((x.shape[0], nodes_num)) #we will evaluate in network class using forward class\n",
    "    print('done with layer forward linear initialisation')\n",
    "    return a\n",
    "\n",
    "def activations_initialisation(x, y, nodes_num, layers_num):\n",
    "    activations = []\n",
    "    for i in range(layers_num):\n",
    "        activations.append('sigmoid')\n",
    "    activations.append('softmax')\n",
    "    print('done with activations initialisation')\n",
    "    return activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Get the MNIST DATASET\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "x, y = mnist['data'], mnist['target']\n",
    "x = x/255\n",
    "y = np.array(y, dtype='int')\n",
    "y = np.eye(10)[y]\n",
    "x_train, x_test, y_train, y_test = x[:60000], x[60000:], y[:60000], y[60000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with weights and biases initialisation\n",
      "done with layer forward linear initialisation\n",
      "done with z initialisation\n",
      "done with activations initialisation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Initialise the parameters\n",
    "nodes_num = 10\n",
    "layers_num = 2\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "weights, biases = weights_and_biases_initialisation(x_train, y_train, nodes_num, layers_num)\n",
    "a = layer_forward_linear_initialisation(x_train, y_train, nodes_num, layers_num)\n",
    "z = z_initialisation(x_train, y_train, nodes_num, layers_num)\n",
    "activations = activations_initialisation(x_train, y_train, nodes_num, layers_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with update parameters\n",
      "Epoch: 0 Loss: 676771.6180387957\n",
      "Accuracy: 0.0\n",
      "(60000, 10) (60000, 10)\n",
      "Layer: 2\n",
      "(60000, 10) (60000, 10)\n",
      "Layer: 1\n",
      "(60000, 10) (60000, 10)\n",
      "Layer: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahil Chaudhary\\AppData\\Local\\Temp\\ipykernel_60456\\2719104196.py:93: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  return np.mean(y_pred == y_true)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (60000,784) (60000,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m weights, biases, loss, accuracy1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[57], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(x, y, weights, biases, a, z, nodes_num, layers_num, activations, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     28\u001b[0m         weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate\u001b[38;5;241m*\u001b[39mdw\n\u001b[0;32m     29\u001b[0m         biases[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate\u001b[38;5;241m*\u001b[39mdb\n\u001b[1;32m---> 30\u001b[0m         dz \u001b[38;5;241m=\u001b[39m \u001b[43mactivation_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weights, biases, loss, accuracy1\n",
      "Cell \u001b[1;32mIn[55], line 11\u001b[0m, in \u001b[0;36mactivation_backward\u001b[1;34m(dz, x, activation)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m activation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(dz\u001b[38;5;241m.\u001b[39mshape, sigmoid(x)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdz\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39msigmoid(x))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m activation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(dz\u001b[38;5;241m.\u001b[39mshape, relu(x)\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (60000,784) (60000,10) "
     ]
    }
   ],
   "source": [
    "\n",
    "weights, biases, loss, accuracy1 = train(x_train, y_train, weights, biases, a, z, nodes_num, layers_num, activations, learning_rate, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot the loss and accuracy\n",
    "plot = plot(x_train, y_train, weights)\n",
    "plot.plot_loss(loss)\n",
    "plot.plot_accuracy(accuracy1)\n",
    "plot.plot(loss, accuracy1)\n",
    "\n",
    "#Make predictions\n",
    "predict = predict(x_test, weights)\n",
    "y_pred = predict.predict(x_test)\n",
    "accuracy2 = accuracy(y_pred, y_test)\n",
    "print('Accuracy:', accuracy2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
