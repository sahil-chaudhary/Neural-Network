{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, w, b):\n",
    "        return np.dot(x, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dz, x, w, b):\n",
    "    #dz is the gradient of the loss function with respect to the output of the linear layer which happens to be same as the input of the activation function\n",
    "    dw = np.dot(x.T, dz)\n",
    "    db = np.sum(dz, axis=0)\n",
    "    dx = np.dot(dz, w.T)\n",
    "    return dx, dw, db\n",
    "\n",
    "def activation_backward(dz, x, activation):\n",
    "    if activation == 'sigmoid':\n",
    "        print(dz.shape, sigmoid(x).shape)\n",
    "        return dz*sigmoid(x)*(1-sigmoid(x))\n",
    "    elif activation == 'relu':\n",
    "        print(dz.shape, relu(x).shape)\n",
    "        return np.where(x>0, dz, 0)\n",
    "    elif activation == 'tanh':\n",
    "        print(dz.shape, tanh(x).shape)\n",
    "        return 1 - np.tanh(x)**2\n",
    "    elif activation == 'softmax':\n",
    "        print(dz.shape, softmax(x).shape)\n",
    "        return dz*softmax(x)*(1-softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class optimiser:\n",
    "    #This will take care of the collection of optimisation algorithms that can be used to update the weights and biases of the neural network. Later, we will compare the performance of different optimisation algorithms on the same neural network and make some conclusions.\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def gradient_descent(self, weights, biases, dw, db):\n",
    "        weights -= self.learning_rate*dw\n",
    "        biases -= self.learning_rate*db\n",
    "        return weights, biases\n",
    "    \n",
    "    def momentum(self, weights, biases, dw, db, beta):\n",
    "        v_w = beta*v_w + (1-beta)*dw\n",
    "        v_b = beta*v_b + (1-beta)*db\n",
    "        weights -= self.learning_rate*v_w\n",
    "        biases -= self.learning_rate*v_b\n",
    "        return weights, biases\n",
    "    \n",
    "    def rmsprop(self, weights, biases, dw, db, beta):\n",
    "        s_w = beta*s_w + (1-beta)*dw**2\n",
    "        s_b = beta*s_b + (1-beta)*db**2\n",
    "        weights -= self.learning_rate*dw/np.sqrt(s_w + 1e-8)\n",
    "        biases -= self.learning_rate*db/np.sqrt(s_b + 1e-8)\n",
    "        return weights, biases\n",
    "    \n",
    "    def adam(self, weights, biases, dw, db, beta1, beta2):\n",
    "        v_w = beta1*v_w + (1-beta1)*dw\n",
    "        v_b = beta1*v_b + (1-beta1)*db\n",
    "        s_w = beta2*s_w + (1-beta2)*dw**2\n",
    "        s_b = beta2*s_b + (1-beta2)*db**2\n",
    "        v_w_corrected = v_w/(1-beta1)\n",
    "        v_b_corrected = v_b/(1-beta1)\n",
    "        s_w_corrected = s_w/(1-beta2)\n",
    "        s_b_corrected = s_b/(1-beta2)\n",
    "        weights -= self.learning_rate*v_w_corrected/np.sqrt(s_w_corrected + 1e-8)\n",
    "        biases -= self.learning_rate*v_b_corrected/np.sqrt(s_b_corrected + 1e-8)\n",
    "        return weights, biases\n",
    "    \n",
    "\n",
    "class plot:\n",
    "    #This class will take care of the plotting of the neural network. This class will have the following methods:\n",
    "    #1. plot_loss : This will plot the loss function of the neural network\n",
    "    #2. plot_accuracy : This will plot the accuracy of the neural network\n",
    "    #3. plot : This will plot both the loss function and the accuracy of the neural network\n",
    "    #4. plot_decision_boundary : This will plot the decision boundary of the neural network. This will be useful for the classification problems. This will help us to visualise how the neural network is making the decision on the basis of the input features.\n",
    "    #5. plot_confusion_matrix : This will plot the confusion matrix of the neural network. This will help us to visualise how the neural network is performing on the basis of the true labels and the predicted labels.\n",
    "    def __init__(self, x, y, parameters):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.parameters = parameters\n",
    "\n",
    "    def plot_loss(self, loss):\n",
    "        plt.plot(loss)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss Function')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_accuracy(self, accuracy):\n",
    "        plt.plot(accuracy)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy')\n",
    "        plt.show()\n",
    "\n",
    "    def plot(self, loss, accuracy):\n",
    "        plt.plot(loss, label='Loss')\n",
    "        plt.plot(accuracy, label='Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss/Accuracy')\n",
    "        plt.title('Loss/Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_decision_boundary(self, x, y, parameters):\n",
    "        pass\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def mean_squared_error(y_pred, y_true):\n",
    "    return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "def cross_entropy_error(y_pred, y_true):\n",
    "    return -np.sum(y_true*np.log(y_pred))\n",
    "\n",
    "\n",
    "def predict(x):\n",
    "    return np.argmax(x, axis=1)\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    return np.mean(y_pred == y_true)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_parameters(x, y, weights, biases, a, z, nodes_num, layers_num, activations):\n",
    "    for i in range(layers_num+1):\n",
    "        a['a_'+str(i)] = linear(z['z_'+str(i)], weights['w_'+str(i)], biases['b_'+str(i)])\n",
    "        if activations[i] == 'sigmoid':\n",
    "            z['z_'+str(i+1)] = sigmoid(a['a_'+str(i)])\n",
    "        elif activations[i] == 'relu':\n",
    "            z['z_'+str(i+1)] = relu(a['a_'+str(i)])\n",
    "        elif activations[i] == 'tanh':\n",
    "            z['z_'+str(i+1)] = tanh(a['a_'+str(i)])\n",
    "        elif activations[i] == 'softmax':\n",
    "            z['z_'+str(i+1)] = softmax(a['a_'+str(i)])\n",
    "    print('done with update parameters')\n",
    "    return a, z\n",
    "\n",
    "def finding_gradients(x, y, weights, biases, a, z, nodes_num, layers_num, activations, optimisers, dw, dz, da, db):\n",
    "    # Assuming the intention was to calculate the gradient of the loss with respect to z at the last layer\n",
    "    dz['dz_'+str(layers_num+1)] = z['z_'+str(layers_num+1)] - y\n",
    "    # If x.shape[0] was meant to be used for normalization or another purpose, it should be assigned or used separately\n",
    "    batch_size = x.shape[0]  # Example of assigning x.shape[0] to a variable for clarity\n",
    "    for i in range(layers_num, -1, -1):\n",
    "        da['da_'+str(i)] = linear_backward(dz['dz_'+str(i+1)], z['z_'+str(i)], weights['w_'+str(i)], biases['b_'+str(i)])[0]\n",
    "        if i!=0:\n",
    "            dz['dz_'+str(i)] = activation_backward(da['da_'+str(i)], a['a_'+str(i)], activations[i])\n",
    "        else:\n",
    "            dz['dz_'+str(i)] = np.dot(da['da_'+str(i)], weights['w_'+str(i)])\n",
    "        dw['dw_'+str(i)], db['db_'+str(i)] = linear_backward(dz['dz_'+str(i)], z['z_'+str(i)], weights['w_'+str(i)], biases['b_'+str(i)])[1:]\n",
    "    return dw, dz, da, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y ,weights, biases, nodes_num, layers_num, activations, loss, optimiser, epochs, learning_rate,a, z, dw, dz, da, db):\n",
    "    #This function will take care of the training of the neural network. This function will have the following steps:\n",
    "    #1. Forward pass : This will take care of the forward pass of the neural network. This will take the input features and the initial weights and biases of the neural network and will return the output of the neural network.\n",
    "    #2. Loss function : This will take care of the loss function of the neural network. This will take the output of the neural network and the true labels and will return the loss of the neural network.\n",
    "    #3. Backward pass : This will\n",
    "    #4. Update parameters : This will take care of the updating of the weights and biases of the neural network. This will take the gradients of the weights and biases and will update the weights and biases of the neural network.\n",
    "    #5. Training loop : This will take care of the training of the neural network. This will take the input features, true labels, initial weights and biases, number of nodes in each layer, number of layers, activation functions, loss function, optimisation algorithm, number of epochs and learning rate as input and will return the trained weights and biases of the neural network.\n",
    "    #6. Plot : This will plot the loss function and the accuracy of the neural network.\n",
    "    #7. Return : This will return the trained weights and biases of the neural network.\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        a, z = update_parameters(x, y, weights, biases, a, z, nodes_num, layers_num, activations)\n",
    "        dw, dz, da, db = finding_gradients(x, y, weights, biases, a, z, nodes_num, layers_num, activations, optimiser, dw, dz, da, db)\n",
    "        for j in range(layers_num+1):\n",
    "            weights['w_'+str(j)], biases['b_'+str(j)] = optimiser.gradient_descent(weights['w_'+str(j)], biases['b_'+str(j)], dw['dw_'+str(j)], db['db_'+str(j)])\n",
    "        loss, accuracy = test(x, y, weights, biases, nodes_num, layers_num, activations, loss, a, z)\n",
    "        print('Epoch :', i, 'Loss :', loss, 'Accuracy :', accuracy)\n",
    "    return weights, biases\n",
    "\n",
    "def test(x, y, weights, biases, nodes_num, layers_num, activations, loss, a, z):\n",
    "    #This function will take care of the testing of the neural network. This function will take the input features, true labels, trained weights and biases, number of nodes in each layer, number of layers and activation functions as input and will return the loss and accuracy of the neural network.\n",
    "    for i in range(layers_num+1):\n",
    "        a['a_'+str(i)] = linear(z['z_'+str(i)], weights['w_'+str(i)], biases['b_'+str(i)])\n",
    "        if activations[i] == 'sigmoid':\n",
    "            z['z_'+str(i+1)] = sigmoid(a['a_'+str(i)])\n",
    "        elif activations[i] == 'relu':\n",
    "            z['z_'+str(i+1)] = relu(a['a_'+str(i)])\n",
    "        elif activations[i] == 'tanh':\n",
    "            z['z_'+str(i+1)] = tanh(a['a_'+str(i)])\n",
    "        elif activations[i] == 'softmax':\n",
    "            z['z_'+str(i+1)] = softmax(a['a_'+str(i)])\n",
    "    loss = cross_entropy_error(z['z_'+str(layers_num+1)], y)\n",
    "    y_pred = predict(z['z_'+str(layers_num+1)])\n",
    "    acc = accuracy(y_pred, y)\n",
    "    return loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weights_and_biases_initialisation(x, y, nodes_num, layers_num):\n",
    "    weights = {}\n",
    "    biases = {}\n",
    "    for i in range(layers_num+1):\n",
    "        print('Layer:', i)\n",
    "        if i == 0:\n",
    "            weights['w_0'] = np.random.randn(x.shape[1], nodes_num)\n",
    "            biases['b_0'] = np.random.randn(1, nodes_num)\n",
    "        elif i == layers_num:\n",
    "            weights['w_'+str(i)] = np.random.randn(nodes_num, y.shape[1])\n",
    "            biases['b_'+str(i)] = np.random.randn(1, y.shape[1])\n",
    "        else:\n",
    "            weights['w_'+str(i)] = np.random.randn(nodes_num, nodes_num)\n",
    "            biases['b_'+str(i)] = np.random.randn(1, nodes_num)\n",
    "    print('done with weights and biases initialisation')\n",
    "    return weights, biases\n",
    "\n",
    "def z_initialisation(x, y, nodes_num, layers_num):\n",
    "    z={}\n",
    "    for i in range(layers_num+2):\n",
    "        print('Layer:', i)\n",
    "        if i == 0:\n",
    "            z['z_0'] = x\n",
    "        else:\n",
    "            z['z_'+str(i)] = np.zeros((x.shape[0], nodes_num))\n",
    "    print('done with z initialisation')\n",
    "    return z\n",
    "\n",
    "\n",
    "def layer_forward_linear_initialisation(x, y, nodes_num, layers_num):\n",
    "    a={}\n",
    "\n",
    "    for i in range(layers_num+1):\n",
    "        print('Layer:', i)\n",
    "        a['a_'+str(i)] = np.zeros((x.shape[0], nodes_num)) #we will evaluate in network class using forward class\n",
    "    print('done with layer forward linear initialisation')\n",
    "    return a\n",
    "\n",
    "def activations_initialisation(x, y, nodes_num, layers_num):\n",
    "    activations = []\n",
    "    for i in range(layers_num):\n",
    "        print('Layer:', i)\n",
    "        activations.append('sigmoid')\n",
    "    activations.append('softmax')\n",
    "    print('done with activations initialisation')\n",
    "    return activations\n",
    "\n",
    "def dz_initialisation(x, y, nodes_num, layers_num):\n",
    "    dz = {}\n",
    "    for i in range(layers_num+1):\n",
    "        print('Layer:', i)\n",
    "        dz['dz_'+str(i)] = np.zeros((x.shape[0], nodes_num))\n",
    "    print('done with dz initialisation')\n",
    "    return dz\n",
    "\n",
    "def da_initialisation(x, y, nodes_num, layers_num):\n",
    "    da = {}\n",
    "    for i in range(layers_num+1):\n",
    "        print('Layer:', i)\n",
    "        da['da_'+str(i)] = np.zeros((x.shape[0], nodes_num))\n",
    "    print('done with da initialisation')\n",
    "    return da\n",
    "\n",
    "def dw_initialisation(x, y, nodes_num, layers_num):\n",
    "    dw = {}\n",
    "    for i in range(layers_num+1):\n",
    "        print('Layer:', i)\n",
    "        dw['dw_'+str(i)] = np.zeros((x.shape[1], nodes_num))\n",
    "    print('done with dw initialisation')\n",
    "    return dw\n",
    "\n",
    "def db_initialisation(x, y, nodes_num, layers_num):\n",
    "    db = {}\n",
    "    for i in range(layers_num+1):\n",
    "        print('Layer:', i)\n",
    "        db['db_'+str(i)] = np.zeros((1, nodes_num))\n",
    "    print('done with db initialisation')\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Get the MNIST DATASET\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "x, y = mnist['data'], mnist['target']\n",
    "x = x/255\n",
    "y = np.array(y, dtype='int')\n",
    "y = np.eye(10)[y]\n",
    "x_train, x_test, y_train, y_test = x[:60000], x[60000:], y[:60000], y[60000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0\n",
      "Layer: 1\n",
      "Layer: 2\n",
      "done with weights and biases initialisation\n",
      "Layer: 0\n",
      "Layer: 1\n",
      "Layer: 2\n",
      "done with layer forward linear initialisation\n",
      "Layer: 0\n",
      "Layer: 1\n",
      "Layer: 2\n",
      "Layer: 3\n",
      "done with z initialisation\n",
      "Layer: 0\n",
      "Layer: 1\n",
      "done with activations initialisation\n",
      "Layer: 0\n",
      "Layer: 1\n",
      "Layer: 2\n",
      "done with dz initialisation\n",
      "Layer: 0\n",
      "Layer: 1\n",
      "Layer: 2\n",
      "done with da initialisation\n",
      "Layer: 0\n",
      "Layer: 1\n",
      "Layer: 2\n",
      "done with dw initialisation\n",
      "Layer: 0\n",
      "Layer: 1\n",
      "Layer: 2\n",
      "done with db initialisation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Initialise the parameters\n",
    "nodes_num = 10\n",
    "layers_num = 2\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "weights, biases = weights_and_biases_initialisation(x_train, y_train, nodes_num, layers_num)\n",
    "a = layer_forward_linear_initialisation(x_train, y_train, nodes_num, layers_num)\n",
    "z = z_initialisation(x_train, y_train, nodes_num, layers_num)\n",
    "activations = activations_initialisation(x_train, y_train, nodes_num, layers_num)\n",
    "dz = dz_initialisation(x_train, y_train, nodes_num, layers_num)\n",
    "da = da_initialisation(x_train, y_train, nodes_num, layers_num)\n",
    "dw = dw_initialisation(x_train, y_train, nodes_num, layers_num)\n",
    "db = db_initialisation(x_train, y_train, nodes_num, layers_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahil Chaudhary\\AppData\\Local\\Temp\\ipykernel_60456\\4176075870.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahil Chaudhary\\AppData\\Local\\Temp\\ipykernel_60456\\4176075870.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahil Chaudhary\\AppData\\Local\\Temp\\ipykernel_60456\\4176075870.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahil Chaudhary\\AppData\\Local\\Temp\\ipykernel_60456\\4176075870.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahil Chaudhary\\AppData\\Local\\Temp\\ipykernel_60456\\4176075870.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahil Chaudhary\\AppData\\Local\\Temp\\ipykernel_60456\\4176075870.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahil Chaudhary\\AppData\\Local\\Temp\\ipykernel_60456\\4176075870.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahil Chaudhary\\AppData\\Local\\Temp\\ipykernel_60456\\4176075870.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahil Chaudhary\\AppData\\Local\\Temp\\ipykernel_60456\\4176075870.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahil Chaudhary\\AppData\\Local\\Temp\\ipykernel_60456\\4176075870.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahil Chaudhary\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n",
      "done with update parameters\n",
      "(60000, 10) (60000, 10)\n",
      "(60000, 10) (60000, 10)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[291], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Train the neural network\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m weights, biases \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_entropy_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Test the neural network\u001b[39;00m\n\u001b[0;32m      5\u001b[0m loss, acc \u001b[38;5;241m=\u001b[39m test(x_test, y_test, weights, biases, nodes_num, layers_num, activations, cross_entropy_error, a, z)\n",
      "Cell \u001b[1;32mIn[287], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(x, y, weights, biases, nodes_num, layers_num, activations, loss, optimiser, epochs, learning_rate, a, z, dw, dz, da, db)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     12\u001b[0m     a, z \u001b[38;5;241m=\u001b[39m update_parameters(x, y, weights, biases, a, z, nodes_num, layers_num, activations)\n\u001b[1;32m---> 13\u001b[0m     dw, dz, da, db \u001b[38;5;241m=\u001b[39m \u001b[43mfinding_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(layers_num\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     15\u001b[0m         weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(j)], biases[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(j)] \u001b[38;5;241m=\u001b[39m optimiser\u001b[38;5;241m.\u001b[39mgradient_descent(weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(j)], biases[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(j)], dw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdw_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(j)], db[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdb_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(j)])\n",
      "Cell \u001b[1;32mIn[286], line 25\u001b[0m, in \u001b[0;36mfinding_gradients\u001b[1;34m(x, y, weights, biases, a, z, nodes_num, layers_num, activations, optimisers, dw, dz, da, db)\u001b[0m\n\u001b[0;32m     23\u001b[0m         dz[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdz_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m activation_backward(da[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mda_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)], a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)], activations[i])\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         dz[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdz_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mda\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mda_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     dw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdw_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)], db[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdb_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m linear_backward(dz[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdz_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)], z[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)], weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)], biases[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)])[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dw, dz, da, db\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train the neural network\n",
    "weights, biases = train(x_train, y_train, weights, biases, nodes_num, layers_num, activations, cross_entropy_error, optimiser(learning_rate), epochs, learning_rate, a, z, dw, dz, da, db)\n",
    "\n",
    "#Test the neural network\n",
    "loss, acc = test(x_test, y_test, weights, biases, nodes_num, layers_num, activations, cross_entropy_error, a, z)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', acc)\n",
    "\n",
    "#Plot the loss and accuracy of the neural network\n",
    "plot = plot(x_train, y_train, weights)\n",
    "plot.plot_loss(loss)\n",
    "plot.plot_accuracy(acc)\n",
    "plot.plot(loss, acc)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
